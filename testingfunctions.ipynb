{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json #for json files\n",
    "import sys #used to close json file\n",
    "import asf_search as asf #ASF search API tool\n",
    "from hyp3_sdk import HyP3 #ASF API tool for sending INSAR jobs\n",
    "import csv #creating a .csv\n",
    "import requests #for checking status of job\n",
    "\n",
    "#import libraries for file management\n",
    "import os \n",
    "from zipfile import ZipFile \n",
    "import shutil\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host='localhost',database = 'postgres', user='postgres',password = 'postgrespw',port=32768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.getcwd()\n",
    "\n",
    "shp = gpd.GeoDataFrame.from_file('geodata/SC-Unit1D.shp')\n",
    "geometry = str(shp['geometry'][0])\n",
    "date = str(shp['Date'][0])\n",
    "JobName = str(shp['Name'][0]).replace(\" \",\"\").replace(\"-\",\"\")\n",
    "print(JobName)\n",
    "d2 = datetime.strptime(date, '%Y-%m-%d')\n",
    "end = str(d2 + timedelta(days = 1)*365)\n",
    "start = str(d2 - timedelta(days = 1)*365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonFile = open('geodata/Login.json')\n",
    "login = json.load(jsonFile)\n",
    "\n",
    "userName = login['UserName']\n",
    "if userName == \"\":\n",
    "    print(\"Please input Username\")\n",
    "    sys.exit()\n",
    "pw = login['Password']\n",
    "if pw ==\"\":\n",
    "    print(\"Please input Password\")\n",
    "    sys.exit()\n",
    "\n",
    "jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an API search based on set parameters\n",
    "searchResults = asf.search(\n",
    "    #search parameters taken from filled out json file\n",
    "    platform = \"Sentinel-1\",\n",
    "    beamMode = \"IW\",\n",
    "    polarization = \"VV+VH\",\n",
    "    intersectsWith = geometry, \n",
    "    processingLevel= \"SLC\",\n",
    "    start = start,\n",
    "    end = end\n",
    ")\n",
    "print('Done')\n",
    "data = searchResults.geojson()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts scenes from job search and creates list of all scene names\n",
    "def get_scene_name(): \n",
    "    count = len(data[\"features\"])                   \n",
    "    scenes = []\n",
    "    dates = []\n",
    "    for x in range(count):\n",
    "        scene = data[\"features\"][x][\"properties\"][\"sceneName\"]\n",
    "        date = data[\"features\"][x][\"properties\"][\"startTime\"]\n",
    "        scenes.append(scene)\n",
    "        dates.append(date)\n",
    "    scenes.reverse()\n",
    "    dates.reverse()\n",
    "    df = pd.DataFrame(list(zip(dates, scenes)),columns =['Dates', 'SceneNames'])\n",
    "    return df\n",
    "scenesList= get_scene_name() #list with all scene names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered = scenesList.drop_duplicates()\n",
    "scenes = list(filtered['SceneNames'])\n",
    "dates = list(filtered['Dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jobs(data,date): \n",
    "    jobs = [] \n",
    "    count = len(data) \n",
    "    for i in range(count-1): \n",
    "        scene1 = data[i]\n",
    "        scene2 = data[i+1]\n",
    "        if date[i+1].endswith('Z'):\n",
    "            sceneDate = datetime.strptime(date[i+1], '%Y-%m-%dT%H:%M:%S.%fZ').date()\n",
    "        else:\n",
    "            sceneDate = datetime.strptime(date[i+1], '%Y-%m-%dT%H:%M:%S.%f').date()\n",
    "        insar_job = HyP3.prepare_insar_job(scene1, scene2, name = JobName, include_displacement_maps=True,include_dem=True,include_look_vectors=True)\n",
    "        jobs.append(insar_job) \n",
    "        pair = scene1+','+scene2\n",
    "        try:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"INSERT INTO project1 (pair, site, reference_date) VALUES(%s, %s, %s)\", (pair, JobName, sceneDate))\n",
    "            conn.commit()\n",
    "            cursor.close()\n",
    "        except:\n",
    "            conn = psycopg2.connect(host='localhost',database = 'postgres', user='postgres',password = 'postgrespw',port=32768)\n",
    "    return jobs \n",
    "jobsList = create_jobs(scenes, dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp3 = HyP3(username = userName, password = pw) #authenticate using ASF credentials\n",
    "batch = hyp3.submit_prepared_jobs(prepared_jobs = jobsList)\n",
    "\n",
    "#Creates New Folder called \"data\" where the jobs will be downloaded\n",
    "output = os.path.join(directory, 'rawData')\n",
    "if not os.path.exists(output):\n",
    "    os.makedirs(output)\n",
    "\n",
    "#Watch and Download job\n",
    "if not batch.complete():\n",
    "    batch = hyp3.watch(batch)\n",
    "#Downloads files into data folder\n",
    "batch.download_files(location = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates List of zip file names\n",
    "zipfiles = os.listdir(output)\n",
    "\n",
    "#Unzips each zip file and deletes the zip\n",
    "for x in range(len(zipfiles)):\n",
    "    file = os.path.join(output, zipfiles[x])\n",
    "    with ZipFile(file, 'r') as zip:\n",
    "        zip.extractall(output)\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = os.path.join(directory, 'rawData')\n",
    "folderList = os.listdir(output)\n",
    "tifList = []\n",
    "#Loop through the folders\n",
    "for x in range(len(folderList)):\n",
    "    folderName = os.path.join(output, folderList[x])\n",
    "    fileList = os.listdir(folderName)\n",
    "    #Loop through files in folder\n",
    "    for file in fileList:\n",
    "        #Select file that ends in vert_disp.tif\n",
    "        if file.endswith(\".tif\"):\n",
    "            fileName = os.path.join(folderName, file)\n",
    "            tifList.append(fileName)\n",
    "        if file.startswith(folderList[x]+'.txt'):\n",
    "            fileName = os.path.join(folderName, file)\n",
    "            f = open(fileName,'r')\n",
    "            lines = f.readlines()\n",
    "            scene1 = lines[0].split(' ')\n",
    "            scene2 = lines[1].split(' ')\n",
    "         \n",
    "            pair = scene1[2].replace('\\n','')+','+scene2[2].replace('\\n','')\n",
    "            try:\n",
    "                cursor = conn.cursor()\n",
    "                cursor.execute(\"SELECT insar_name from project1 where insar_name = %s\",(folderList[x],))\n",
    "                row = cursor.fetchall()\n",
    "                if row == []:\n",
    "                    cursor.execute(\"UPDATE project1 SET insar_name = %s where pair = %s\",(folderList[x],pair))\n",
    "                    conn.commit()\n",
    "                cursor.close()\n",
    "            except:\n",
    "                conn = psycopg2.connect(host='localhost',database = 'postgres', user='postgres',password = 'postgrespw',port=32768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries for dealing with geotifs\n",
    "from osgeo import gdal,ogr\n",
    "import rasterio as rio\n",
    "\n",
    "#Math and plotting\n",
    "import numpy as np\n",
    "from scipy.ndimage import maximum_filter\n",
    "import earthpy.plot as ep\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rio.open(tifList[0])\n",
    "boundingBox = dataset.bounds\n",
    "left = boundingBox.left\n",
    "right = boundingBox.right\n",
    "top = boundingBox.top\n",
    "bottom = boundingBox.bottom\n",
    "print(boundingBox)\n",
    "for x in range(1,len(tifList)):\n",
    "    dataset = rio.open(tifList[x])\n",
    "    boundingBox = dataset.bounds\n",
    "    if boundingBox.left < left:\n",
    "        left = boundingBox.left\n",
    "    if boundingBox.right < right:\n",
    "        right = boundingBox.right\n",
    "    if boundingBox.top < top:\n",
    "        top = boundingBox.top\n",
    "    if boundingBox.bottom < bottom:\n",
    "        bottom = boundingBox.bottom\n",
    "    print(left,bottom,right,top)\n",
    "minbbox = (left,bottom,right,top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crop the water mask incase it does not have the min bounds\n",
    "#For loop to crop all the tif files to the min bounds\n",
    "#all files need to be the same height and width to work with them in numpy\n",
    "opts = gdal.WarpOptions(options=['tr'], outputBounds=minbbox, format=\"GTiff\")\n",
    "for geoTif in tifList:\n",
    "    dataset = gdal.Open(geoTif)\n",
    "    newfile = gdal.Warp(geoTif.split('.')[0]+'_crop.tif', dataset , options=opts)\n",
    "    dataset = None #close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from mintpy.cli import tsview\n",
    "from mintpy.cli import plot_transection\n",
    "import ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runMinty = subprocess.run([\"smallbaselineApp.py\",\"mintpyConfigFile.txt\"])\n",
    "getVelocity = subprocess.run([\"smallbaselineApp.py\", \"--dostep\", \"velocity\", \"mintpyConfigFile.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "ts_file = os.path.expanduser('timeseries.h5')\n",
    "cmd = f'{ts_file} --yx 220 300 --figsize 9 3'\n",
    "tsview.main(cmd.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel_file = os.path.expanduser('avgPhaseVelocity.h5')\n",
    "cmd = f'{vel_file} --figsize 10 5'\n",
    "plot_transection.main(cmd.split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b4d75ac280b6c7c3aa43866cb82dc88915409b55fec83a093dd0284cb58708e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
